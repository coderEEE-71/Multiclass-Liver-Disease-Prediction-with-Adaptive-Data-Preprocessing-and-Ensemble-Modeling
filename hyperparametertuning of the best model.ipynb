{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data = pd.read_excel(r\"D:\\user interface\\final_data_first_outlier_then_4log.xlsx\")\n",
    "data.drop([\"Unnamed: 0\"], axis=1,inplace=True)\n",
    "\n",
    "X_train = data.drop(['Category','Sex','CREA'],axis=1)\n",
    "y_train = data['Category']\n",
    "# Save the column names\n",
    "column_names = X_train.columns.tolist()\n",
    "\n",
    "# Standardize the features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "\n",
    "# Create an instance of the SMOTE class\n",
    "smote = SMOTE()\n",
    "\n",
    "# Generate X_resampled and y_resampled\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Selecting a specified number of samples for each category for testing\n",
    "num_samples_per_category = 100  # Specify the desired number of samples for each category\n",
    "\n",
    "test_indices = []\n",
    "for category in np.unique(y_resampled):\n",
    "    category_indices = np.where(y_resampled == category)[0]\n",
    "    category_indices = category_indices[:num_samples_per_category]\n",
    "    test_indices.extend(category_indices)\n",
    "\n",
    "# Resetting the index of X_resampled and y_resampled\n",
    "X_resampled = pd.DataFrame(X_resampled, columns=column_names)\n",
    "y_resampled = pd.Series(y_resampled)\n",
    "\n",
    "# Creating a new set of indices for test_indices\n",
    "new_test_indices = []\n",
    "for index in test_indices:\n",
    "    new_test_indices.append(X_resampled.index[index])\n",
    "\n",
    "# Extracting the corresponding samples for testing\n",
    "X_test = X_resampled.loc[new_test_indices]\n",
    "y_test = y_resampled[new_test_indices]\n",
    "\n",
    "# Dropping the rows for testing samples from X_resampled and y_resampled\n",
    "X_train = X_resampled.drop(index=new_test_indices)\n",
    "y_train = y_resampled.drop(index=new_test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# building the stack lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "xg = XGBClassifier()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "xg.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Define hyperparameter grid for the final estimator (Logistic Regression)\\nfinal_estimator_params = {\\n    'final_estimator__penalty': ['l1', 'l2', 'elasticnet', 'none'],\\n    'final_estimator__C': np.logspace(-4, 4, 20),\\n    'final_estimator__solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],\\n    'final_estimator__max_iter': [50, 100, 250, 500]\\n}\\n\\n# Perform RandomizedSearchCV for the final estimator\\nfinal_estimator_search = RandomizedSearchCV(\\n    stack_model,\\n    final_estimator_params,\\n    cv=7,\\n    return_train_score=False,\\n    n_jobs=-1\\n)\\n\\nfinal_estimator_search.fit(X_train, y_train)\\n\\n# Print the results for the final estimator\\nprint({\\n    'model': 'logistic_regression',\\n    'best_score': final_estimator_search.best_score_,\\n    'best_params': final_estimator_search.best_params_\\n})\\n\\n\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Your previous code for defining base classifiers\n",
    "\n",
    "# Create the StackingClassifier\n",
    "estimator_list = [\n",
    "    ('rf', rf),\n",
    "    ('svm', svm),\n",
    "    ('xg', xg),\n",
    "    ('dt', dt)\n",
    "]\n",
    "\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=estimator_list,\n",
    "    final_estimator=LogisticRegression(max_iter=100000)\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid for the final estimator (Logistic Regression)\n",
    "final_estimator_params = {\n",
    "    'final_estimator__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'final_estimator__C': np.logspace(-4, 4, 20),\n",
    "    'final_estimator__solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],\n",
    "    'final_estimator__max_iter': [50, 100, 250, 500]\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV for the final estimator\n",
    "final_estimator_search = GridSearchCV(\n",
    "    stack_model,\n",
    "    final_estimator_params,\n",
    "    cv=7,\n",
    "    return_train_score=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "final_estimator_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the results for the final estimator\n",
    "print({\n",
    "    'model': 'logistic_regression',\n",
    "    'best_score': final_estimator_search.best_score_,\n",
    "    'best_params': final_estimator_search.best_params_\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find accuracy of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack model train Accuracy : 1.0\n",
      "test Accuracy : 1.0\n",
      "Micro-average Precision: 1.0\n",
      "Micro-average Recall: 1.0\n",
      "Macro-average Precision: 1.0\n",
      "Macro-average Recall: 1.0\n",
      "Mean Absolute Error (MAE): 0.0\n",
      "Root Mean Squared Error (RMSE): 0.0\n"
     ]
    }
   ],
   "source": [
    "stack_model.fit(X_train, y_train)\n",
    "stack_pred = stack_model.predict(X_test)\n",
    "y_pred=stack_model.predict(X_train)\n",
    "\n",
    "print(f\"stack model train Accuracy : {accuracy_score(y_train, y_pred)}\")\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_micro = precision_score(y_test, stack_pred, average='micro')\n",
    "recall_micro = recall_score(y_test, stack_pred, average='micro')\n",
    "\n",
    "precision_macro = precision_score(y_test, stack_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, stack_pred, average='macro')\n",
    "print(f\"test Accuracy : {accuracy_score(y_test, stack_pred)}\")\n",
    "print(f\"Micro-average Precision: {precision_micro}\")\n",
    "print(f\"Micro-average Recall: {recall_micro}\")\n",
    "print(f\"Macro-average Precision: {precision_macro}\")\n",
    "print(f\"Macro-average Recall: {recall_macro}\")\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "mae = mean_absolute_error(y_test, stack_pred)\n",
    "rmse = mean_squared_error(y_test,stack_pred, squared=False)  # squared=False gives RMSE\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For hyperparameter tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest test Accuracy : 0.998\n",
      "test Accuracy : 0.998\n",
      "Micro-average Precision: 0.998\n",
      "Micro-average Recall: 0.998\n",
      "Macro-average Precision: 0.998019801980198\n",
      "Macro-average Recall: 0.998\n",
      "Mean Absolute Error (MAE): 0.002\n",
      "Root Mean Squared Error (RMSE): 0.044721359549995794\n"
     ]
    }
   ],
   "source": [
    "solver = 'sag'\n",
    "penalty = 'none'\n",
    "max_iterations = 500\n",
    "c_value = 0.0001\n",
    "stack_model2 = StackingClassifier(\n",
    "    estimators=estimator_list,\n",
    "    final_estimator=LogisticRegression(solver=solver, penalty=penalty, max_iter=max_iterations, C=c_value)\n",
    "\n",
    ")\n",
    "stack_model2.fit(X_train, y_train)\n",
    "stack_pred2 = stack_model2.predict(X_test)\n",
    "print(f\"random forest test Accuracy : {accuracy_score(y_test, stack_pred2)}\")\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_micro = precision_score(y_test, stack_pred2, average='micro')\n",
    "recall_micro = recall_score(y_test, stack_pred2, average='micro')\n",
    "\n",
    "precision_macro = precision_score(y_test, stack_pred2, average='macro')\n",
    "recall_macro = recall_score(y_test, stack_pred2, average='macro')\n",
    "print(f\"test Accuracy : {accuracy_score(y_test, stack_pred2)}\")\n",
    "print(f\"Micro-average Precision: {precision_micro}\")\n",
    "print(f\"Micro-average Recall: {recall_micro}\")\n",
    "print(f\"Macro-average Precision: {precision_macro}\")\n",
    "print(f\"Macro-average Recall: {recall_macro}\")\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "mae = mean_absolute_error(y_test, stack_pred2)\n",
    "rmse = mean_squared_error(y_test,stack_pred2, squared=False)  # squared=False gives RMSE\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For xgboost test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest test Accuracy : 0.994\n",
      "test Accuracy : 0.994\n",
      "Micro-average Precision: 0.994\n",
      "Micro-average Recall: 0.994\n",
      "Macro-average Precision: 0.9941747572815534\n",
      "Macro-average Recall: 0.994\n",
      "Mean Absolute Error (MAE): 0.012\n",
      "Root Mean Squared Error (RMSE): 0.15491933384829668\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
