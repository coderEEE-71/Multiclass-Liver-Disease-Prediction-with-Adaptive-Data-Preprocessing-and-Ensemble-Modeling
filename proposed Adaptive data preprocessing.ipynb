{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Category', 'Age', 'Sex', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE',\n",
      "       'CHOL', 'CREA', 'GGT', 'PROT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "data = pd.read_csv(r\"D:\\all python works\\for thesis\\HepatitisCdata.csv\")\n",
    "\n",
    "\n",
    "data.drop([ \"Unnamed: 0\"], axis=1, inplace=True)\n",
    "print(data.columns)\n",
    "\n",
    "data.replace(to_replace=['0=Blood Donor', '0s=suspect Blood Donor', '1=Hepatitis',\n",
    "       '2=Fibrosis', '3=Cirrhosis'],\n",
    "             value=[0,1,2,3,4],inplace=True)\n",
    "\n",
    "data.replace(to_replace=['m','f'],\n",
    "             value=[0,1],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proporsed Adaptive Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 615 entries, 0 to 563\n",
      "Data columns (total 13 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Category  615 non-null    int64  \n",
      " 1   Age       615 non-null    int64  \n",
      " 2   Sex       615 non-null    int64  \n",
      " 3   ALB       615 non-null    float64\n",
      " 4   ALP       615 non-null    float64\n",
      " 5   ALT       615 non-null    float64\n",
      " 6   AST       615 non-null    float64\n",
      " 7   BIL       615 non-null    float64\n",
      " 8   CHE       615 non-null    float64\n",
      " 9   CHOL      615 non-null    float64\n",
      " 10  CREA      615 non-null    float64\n",
      " 11  GGT       615 non-null    float64\n",
      " 12  PROT      615 non-null    float64\n",
      "dtypes: float64(10), int64(3)\n",
      "memory usage: 67.3 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahadk\\AppData\\Local\\Temp\\ipykernel_25068\\1630033749.py:57: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '46.966037735849056' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data_category0.loc[outliers, column] = data_category0.loc[~outliers, column].mean()\n",
      "C:\\Users\\ahadk\\AppData\\Local\\Temp\\ipykernel_25068\\1630033749.py:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '46.966037735849056' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[index] = data_category0.loc[index]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "               #handling missing values of cirrosis\n",
    "data_cirrhosis=data[data['Category']== 4]\n",
    "from sklearn.impute import SimpleImputer\n",
    "si = SimpleImputer(missing_values = np.nan, strategy ='mean')\n",
    "si = si.fit(data_cirrhosis.iloc[:,1:13])\n",
    "data_cirrhosis.iloc[:,1:13]= si.transform(data_cirrhosis.iloc[:,1:13])\n",
    "\n",
    "               #for blood donor\n",
    "data_blood_donor=data[data['Category']==0]\n",
    "from sklearn.impute import SimpleImputer\n",
    "si = SimpleImputer(missing_values = np.nan, strategy ='mean')\n",
    "si = si.fit(data_blood_donor.iloc[:,1:13])\n",
    "data_blood_donor.iloc[:,1:13]= si.transform(data_blood_donor.iloc[:,1:13])#set_index was not used so needed to start from column 3\n",
    "\n",
    "               #for heaptitis\n",
    "data_hepatitis=data[data['Category']==2]\n",
    "from sklearn.impute import SimpleImputer\n",
    "si = SimpleImputer(missing_values = np.nan, strategy ='mean')\n",
    "si = si.fit(data_hepatitis.iloc[:,1:13])\n",
    "data_hepatitis.iloc[:,1:13]= si.transform(data_hepatitis.iloc[:,1:13])#set_index was not used so needed to start from column 3\n",
    "\n",
    "           #for fibrosis\n",
    "data_Fibrosis=data[data['Category']==3]\n",
    "from sklearn.impute import SimpleImputer\n",
    "si = SimpleImputer(missing_values = np.nan, strategy ='mean')\n",
    "si = si.fit(data_Fibrosis.iloc[:,1:13])\n",
    "data_Fibrosis.iloc[:,1:13]= si.transform(data_Fibrosis.iloc[:,1:13])\n",
    "\n",
    "               #for suspect blood donor\n",
    "data_suspect=data[data['Category']== 1]\n",
    "\n",
    "data=pd.concat([data_blood_donor,data_suspect,data_cirrhosis,data_Fibrosis,data_hepatitis])\n",
    "data.info()\n",
    "\n",
    "# Assuming data is your dataframe\n",
    "# First, let's filter only category 0\n",
    "data_category0 = data[data['Category'] == 0]\n",
    "\n",
    "# Now, let's detect and replace outliers in each column\n",
    "new_columns = [ 'Age', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']\n",
    "\n",
    "outliers_exist = True\n",
    "while outliers_exist:\n",
    "    outliers_exist = False\n",
    "    for column in new_columns:\n",
    "        Q1 = data_category0[column].quantile(0.25)\n",
    "        Q3 = data_category0[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Define outliers\n",
    "        outliers = ((data_category0[column] < (Q1 - 1.5 * IQR)) | (data_category0[column] > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "        if outliers.any():\n",
    "            outliers_exist = True\n",
    "\n",
    "            # Replace outliers with the mean of the non-outliers values\n",
    "            data_category0.loc[outliers, column] = data_category0.loc[~outliers, column].mean()\n",
    "\n",
    "# Now, replace the original category 0 with the cleaned one\n",
    "for index in data_category0.index:\n",
    "    data.loc[index] = data_category0.loc[index]\n",
    "                                       #now log transform\n",
    "data['Logged ALT']=np.log(data['ALT'])\n",
    "data['Logged AST']=np.log(data['AST'])\n",
    "data['DLogged AST']=np.log(data['Logged AST'])\n",
    "data['TLogged AST']=np.log(data['DLogged AST']) #Further log transform of AST will result the values to be negative so it will not be logtransformable\n",
    "data['Logged BIL']=np.log(data['BIL'])\n",
    "data['Logged GGT']=np.log(data['GGT'])\n",
    "data['DLogged GGT']=np.log(data['Logged GGT']) #Further log transform of GGT will result the values to be negative so it will not be logtransformable\n",
    "\n",
    "\n",
    "new_columns=['Age','Sex','ALB','CHE','CHOL','PROT','CREA','ALP','Logged ALT','TLogged AST','Logged BIL','DLogged GGT','Category']\n",
    "data[new_columns]\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set the style to white\n",
    "sns.set_style(\"whitegrid\")\n",
    "data.drop(['ALT'], axis=1,inplace=True)\n",
    "data.drop(['AST'], axis=1,inplace=True)\n",
    "data.drop(['Logged AST'], axis=1,inplace=True)\n",
    "data.drop(['DLogged AST'], axis=1,inplace=True)\n",
    "data.drop(['BIL'], axis=1,inplace=True)\n",
    "data.drop(['GGT'], axis=1,inplace=True)\n",
    "data.drop(['Logged GGT'], axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model and feature selection and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgBoost Accuracy : 0.9479773100462755\n",
      "xgBoost Precision : 0.7833787640930497\n",
      "xgBoost Recall : 0.6719047619047619\n",
      "xgBoost F1-Score : 0.7002193448249291\n",
      "xgBoost MAE : 0.07314524555903866\n",
      "xgBoost RMSE : 0.358297878526625\n",
      "\n",
      "svm Accuracy : 0.9544334975369458\n",
      "svm Precision : 0.8434184862756292\n",
      "svm Recall : 0.6258145363408522\n",
      "svm F1-Score : 0.63115858144673\n",
      "svm MAE : 0.08462083893118377\n",
      "svm RMSE : 0.44197999993927367\n",
      "\n",
      "logistic regression Accuracy : 0.962569040155247\n",
      "logistic regression Precision : 0.9008740024291562\n",
      "logistic regression Recall : 0.7657142857142857\n",
      "logistic regression F1-Score : 0.7613955259523164\n",
      "logistic regression MAE : 0.06185624720107479\n",
      "logistic regression RMSE : 0.3564443086317881\n",
      "\n",
      "random forest Accuracy : 0.9544894760412\n",
      "random forest Precision : 0.7949402977974406\n",
      "random forest Recall : 0.7119047619047619\n",
      "random forest F1-Score : 0.7653040453718143\n",
      "random forest MAE : 0.060158232572025676\n",
      "random forest RMSE : 0.3123926894419564\n",
      "\n",
      "decision tree Accuracy : 0.9235333631885355\n",
      "decision tree Precision : 0.5919589933875649\n",
      "decision tree Recall : 0.5678195488721804\n",
      "decision tree F1-Score : 0.7504268374016274\n",
      "decision tree MAE : 0.11876772652634722\n",
      "decision tree RMSE : 0.4492451083077129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train=data.drop(columns=['Category','Sex','CREA'],axis=1)\n",
    "y_train=data['Category']\n",
    "\n",
    "# Standardize the features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "\n",
    "                  #Extract the test data and data balancing\n",
    "'''\n",
    "\n",
    "# Save the column names\n",
    "column_names = X_train.columns.tolist()\n",
    "\n",
    "# Standardize the features\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "\n",
    "# Create an instance of the SMOTE class\n",
    "smote = SMOTE()\n",
    "\n",
    "# Generate X_resampled and y_resampled\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Selecting a specified number of samples for each category for testing\n",
    "num_samples_per_category = 100  # Specify the desired number of samples for each category\n",
    "\n",
    "test_indices = []\n",
    "for category in np.unique(y_resampled):\n",
    "    category_indices = np.where(y_resampled == category)[0]\n",
    "    category_indices = category_indices[:num_samples_per_category]\n",
    "    test_indices.extend(category_indices)\n",
    "\n",
    "# Resetting the index of X_resampled and y_resampled\n",
    "X_resampled = pd.DataFrame(X_resampled, columns=column_names)\n",
    "y_resampled = pd.Series(y_resampled)\n",
    "\n",
    "# Creating a new set of indices for test_indices\n",
    "new_test_indices = []\n",
    "for index in test_indices:\n",
    "    new_test_indices.append(X_resampled.index[index])\n",
    "\n",
    "# Extracting the corresponding samples for testing\n",
    "X_test = X_resampled.loc[new_test_indices]\n",
    "y_test = y_resampled[new_test_indices]\n",
    "\n",
    "# Dropping the rows for testing samples from X_resampled and y_resampled\n",
    "X_train = X_resampled.drop(index=new_test_indices)\n",
    "y_train = y_resampled.drop(index=new_test_indices)'''\n",
    "\n",
    "\n",
    "#model building\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "xg = XGBClassifier()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter=10000)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming you have defined and initialized the model lr, X_train, and y_train\n",
    "\n",
    "# Create StratifiedKFold with 10 folds\n",
    "#cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have defined and initialized the models xg, X_train, and y_train\n",
    "\n",
    "# xgBoost\n",
    "xg_acc = cross_val_score(estimator=xg, X=X_train, y=y_train, cv=7, scoring='accuracy')\n",
    "xg_prec_scorer = make_scorer(precision_score, average='macro', zero_division=1)\n",
    "xg_prec = cross_val_score(estimator=xg, X=X_train, y=y_train, cv=7, scoring=xg_prec_scorer)\n",
    "xg_rec_scorer = make_scorer(recall_score, average='macro', zero_division=1)\n",
    "xg_rec = cross_val_score(estimator=xg, X=X_train, y=y_train, cv=7, scoring=xg_rec_scorer)\n",
    "xg_f1_scorer = make_scorer(f1_score, average='macro', zero_division=1)\n",
    "xg_f1 = cross_val_score(estimator=xg, X=X_train, y=y_train, cv=7, scoring=xg_f1_scorer)\n",
    "xg_mae = cross_val_score(estimator=xg, X=X_train, y=y_train, cv=7, scoring='neg_mean_absolute_error')\n",
    "xg_rmse = cross_val_score(estimator=xg, X=X_train, y=y_train, cv=7, scoring='neg_mean_squared_error')\n",
    "actual_mae = abs(xg_mae.mean())\n",
    "print(f\"xgBoost Accuracy : {xg_acc.mean()}\")\n",
    "print(f\"xgBoost Precision : {xg_prec.mean()}\")\n",
    "print(f\"xgBoost Recall : {xg_rec.mean()}\")\n",
    "print(f\"xgBoost F1-Score : {xg_f1.mean()}\")\n",
    "print(f\"xgBoost MAE : {actual_mae.mean()}\")\n",
    "print(f\"xgBoost RMSE : {sqrt(-xg_rmse.mean())}\\n\")\n",
    "\n",
    "# SVM\n",
    "svm_acc = cross_val_score(estimator=svm, X=X_train, y=y_train, cv=7, scoring='accuracy')\n",
    "svm_prec_scorer = make_scorer(precision_score, average='macro', zero_division=1)\n",
    "svm_prec = cross_val_score(estimator=svm, X=X_train, y=y_train, cv=7, scoring=svm_prec_scorer)\n",
    "svm_rec_scorer = make_scorer(recall_score, average='macro', zero_division=1)\n",
    "svm_rec = cross_val_score(estimator=svm, X=X_train, y=y_train, cv=7, scoring=svm_rec_scorer)\n",
    "svm_f1_scorer = make_scorer(f1_score, average='macro', zero_division=1)\n",
    "svm_f1 = cross_val_score(estimator=svm, X=X_train, y=y_train, cv=7, scoring=svm_f1_scorer)\n",
    "svm_mae = cross_val_score(estimator=svm, X=X_train, y=y_train, cv=7, scoring='neg_mean_absolute_error')\n",
    "svm_rmse = cross_val_score(estimator=svm, X=X_train, y=y_train, cv=7, scoring='neg_mean_squared_error')\n",
    "actual_mae = abs(svm_mae.mean())\n",
    "print(f\"svm Accuracy : {svm_acc.mean()}\")\n",
    "print(f\"svm Precision : {svm_prec.mean()}\")\n",
    "print(f\"svm Recall : {svm_rec.mean()}\")\n",
    "print(f\"svm F1-Score : {svm_f1.mean()}\")\n",
    "print(f\"svm MAE : {actual_mae.mean()}\")\n",
    "print(f\"svm RMSE : {sqrt(-svm_rmse.mean())}\\n\")\n",
    "\n",
    "# lr\n",
    "lr_acc = cross_val_score(estimator=lr, X=X_train, y=y_train, cv=7, scoring='accuracy')\n",
    "lr_prec_scorer = make_scorer(precision_score, average='macro', zero_division=1)\n",
    "lr_prec = cross_val_score(estimator=lr, X=X_train, y=y_train, cv=7, scoring=lr_prec_scorer)\n",
    "lr_rec_scorer = make_scorer(recall_score, average='macro', zero_division=1)\n",
    "lr_rec = cross_val_score(estimator=lr, X=X_train, y=y_train, cv=7, scoring=lr_rec_scorer)\n",
    "lr_f1_scorer = make_scorer(f1_score, average='macro', zero_division=1)\n",
    "lr_f1 = cross_val_score(estimator=lr, X=X_train, y=y_train, cv=7, scoring=lr_f1_scorer)\n",
    "lr_mae = cross_val_score(estimator=lr, X=X_train, y=y_train, cv=7, scoring='neg_mean_absolute_error')\n",
    "lr_rmse = cross_val_score(estimator=lr, X=X_train, y=y_train, cv=7, scoring='neg_mean_squared_error')\n",
    "actual_mae = abs(lr_mae.mean())\n",
    "print(f\"logistic regression Accuracy : {lr_acc.mean()}\")\n",
    "print(f\"logistic regression Precision : {lr_prec.mean()}\")\n",
    "print(f\"logistic regression Recall : {lr_rec.mean()}\")\n",
    "print(f\"logistic regression F1-Score : {lr_f1.mean()}\")\n",
    "print(f\"logistic regression MAE : {actual_mae.mean()}\")\n",
    "print(f\"logistic regression RMSE : {sqrt(-lr_rmse.mean())}\\n\")\n",
    "\n",
    "rf_acc = cross_val_score(estimator=rf, X=X_train, y=y_train, cv=7, scoring='accuracy')\n",
    "rf_prec_scorer = make_scorer(precision_score, average='macro', zero_division=1)\n",
    "rf_prec = cross_val_score(estimator=rf, X=X_train, y=y_train, cv=7, scoring=rf_prec_scorer)\n",
    "rf_rec_scorer = make_scorer(recall_score, average='macro', zero_division=1)\n",
    "rf_rec = cross_val_score(estimator=rf, X=X_train, y=y_train, cv=7, scoring=rf_rec_scorer)\n",
    "rf_f1_scorer = make_scorer(f1_score, average='macro', zero_division=1)\n",
    "rf_f1 = cross_val_score(estimator=rf, X=X_train, y=y_train, cv=7, scoring=rf_f1_scorer)\n",
    "rf_mae = cross_val_score(estimator=rf, X=X_train, y=y_train, cv=7, scoring='neg_mean_absolute_error')\n",
    "rf_rmse = cross_val_score(estimator=rf, X=X_train, y=y_train, cv=7, scoring='neg_mean_squared_error')\n",
    "actual_mae = abs(rf_mae.mean())\n",
    "print(f\"random forest Accuracy : {rf_acc.mean()}\")\n",
    "print(f\"random forest Precision : {rf_prec.mean()}\")\n",
    "print(f\"random forest Recall : {rf_rec.mean()}\")\n",
    "print(f\"random forest F1-Score : {rf_f1.mean()}\")\n",
    "print(f\"random forest MAE : {actual_mae.mean()}\")\n",
    "print(f\"random forest RMSE : {sqrt(-rf_rmse.mean())}\\n\")\n",
    "\n",
    "dt_acc = cross_val_score(estimator=dt, X=X_train, y=y_train, cv=7, scoring='accuracy')\n",
    "dt_prec_scorer = make_scorer(precision_score, average='macro', zero_division=1)\n",
    "dt_prec = cross_val_score(estimator=dt, X=X_train, y=y_train, cv=7, scoring=dt_prec_scorer)\n",
    "dt_rec_scorer = make_scorer(recall_score, average='macro', zero_division=1)\n",
    "dt_rec = cross_val_score(estimator=dt, X=X_train, y=y_train, cv=7, scoring=dt_rec_scorer)\n",
    "dt_f1_scorer = make_scorer(f1_score, average='macro', zero_division=1)\n",
    "dt_f1 = cross_val_score(estimator=dt, X=X_train, y=y_train, cv=7, scoring=dt_f1_scorer)\n",
    "dt_mae = cross_val_score(estimator=dt, X=X_train, y=y_train, cv=7, scoring='neg_mean_absolute_error')\n",
    "dt_rmse = cross_val_score(estimator=dt, X=X_train, y=y_train, cv=7, scoring='neg_mean_squared_error')\n",
    "actual_mae = abs(dt_mae.mean())\n",
    "print(f\"decision tree Accuracy : {dt_acc.mean()}\")\n",
    "print(f\"decision tree Precision : {dt_prec.mean()}\")\n",
    "print(f\"decision tree Recall : {dt_rec.mean()}\")\n",
    "print(f\"decision tree F1-Score : {dt_f1.mean()}\")\n",
    "print(f\"decision tree MAE : {actual_mae.mean()}\")\n",
    "print(f\"decision tree RMSE : {sqrt(-dt_rmse.mean())}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensample model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensamble logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked model Accuracy : 0.9528455284552845\n",
      "stacked model Precision : 0.7545291005291005\n",
      "stacked model Recall : 0.6826666666666668\n",
      "stacked model F1-Score : 0.7472878461585264\n",
      "stacked MAE : 0.07154471544715449\n",
      "stacked RMSE : 0.3651483716701107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xg.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#lr = LogisticRegression(max_iter=10000)\n",
    "#lr.fit(X_train, y_train)\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimator_list=[\n",
    "    ('rf',rf),\n",
    "    ('svm',svm),\n",
    "    \n",
    "    ('xg',xg),\n",
    "    ('dt',dt)\n",
    "    ]\n",
    "stack_model=StackingClassifier(\n",
    "    estimators=estimator_list,final_estimator=LogisticRegression(max_iter=10000)\n",
    ")\n",
    "stack_acc = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=5, scoring='accuracy')\n",
    "stack_prec_scorer = make_scorer(precision_score, average='macro', zero_division=1)\n",
    "stack_prec = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=5, scoring=stack_prec_scorer)\n",
    "stack_rec_scorer = make_scorer(recall_score, average='macro', zero_division=1)\n",
    "stack_rec = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=5, scoring=stack_rec_scorer)\n",
    "stack_f1_scorer = make_scorer(f1_score, average='macro', zero_division=1)\n",
    "stack_f1 = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=5, scoring=stack_f1_scorer)\n",
    "stack_mae = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "stack_rmse = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "actual_mae = abs(stack_mae.mean())\n",
    "print(f\"stacked model Accuracy : {stack_acc.mean()}\")\n",
    "print(f\"stacked model Precision : {stack_prec.mean()}\")\n",
    "print(f\"stacked model Recall : {stack_rec.mean()}\")\n",
    "print(f\"stacked model F1-Score : {stack_f1.mean()}\")\n",
    "print(f\"stacked MAE : {actual_mae.mean()}\")\n",
    "print(f\"stacked RMSE : {sqrt(-stack_rmse.mean())}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensambled xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked model Accuracy : 0.9593036274070758\n",
      "stacked model Precision : 0.7516388373531229\n",
      "stacked model Recall : 0.7754385964912281\n",
      "stacked model F1-Score : 0.8213444739855638\n",
      "stacked MAE : 0.0601768920734438\n",
      "stacked RMSE : 0.3228208720331013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(max_iter=100000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "estimator_list=[\n",
    "    ('rf',rf),\n",
    "    ('svm',svm),\n",
    "    ('lr',lr),\n",
    "    ('dt',dt)\n",
    "    ]\n",
    "stack_model=StackingClassifier(\n",
    "    estimators=estimator_list,final_estimator= XGBClassifier()\n",
    "\n",
    ")\n",
    "stack_acc = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=7, scoring='accuracy')\n",
    "stack_prec_scorer = make_scorer(precision_score, average='macro', zero_division=1)\n",
    "stack_prec = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=7, scoring=stack_prec_scorer)\n",
    "stack_rec_scorer = make_scorer(recall_score, average='macro', zero_division=1)\n",
    "stack_rec = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=7, scoring=stack_rec_scorer)\n",
    "stack_f1_scorer = make_scorer(f1_score, average='macro', zero_division=1)\n",
    "stack_f1 = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=7, scoring=stack_f1_scorer)\n",
    "stack_mae = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=7, scoring='neg_mean_absolute_error')\n",
    "stack_rmse = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=7, scoring='neg_mean_squared_error')\n",
    "actual_mae = abs(stack_mae.mean())\n",
    "print(f\"stacked model Accuracy : {stack_acc.mean()}\")\n",
    "print(f\"stacked model Precision : {stack_prec.mean()}\")\n",
    "print(f\"stacked model Recall : {stack_rec.mean()}\")\n",
    "print(f\"stacked model F1-Score : {stack_f1.mean()}\")\n",
    "print(f\"stacked MAE : {actual_mae.mean()}\")\n",
    "print(f\"stacked RMSE : {sqrt(-stack_rmse.mean())}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensamble random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked model Accuracy : 0.9609456635318703\n",
      "stacked model Precision : 0.8227011084153941\n",
      "stacked model Recall : 0.8095238095238095\n",
      "stacked model F1-Score : 0.8201904847960689\n",
      "stacked MAE : 0.048831915211225554\n",
      "stacked RMSE : 0.28815756334103637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "xg = XGBClassifier()\n",
    "xg.fit(X_train, y_train)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(max_iter=100000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "estimator_list=[\n",
    "    ('xg',xg),\n",
    "    ('svm',svm),\n",
    "    ('lr',lr),\n",
    "    ('dt',dt)\n",
    "    ]\n",
    "stack_model=StackingClassifier(\n",
    "    estimators=estimator_list,final_estimator= RandomForestClassifier()\n",
    "\n",
    ")\n",
    "stack_acc = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=7, scoring='accuracy')\n",
    "stack_prec_scorer = make_scorer(precision_score, average='macro', zero_division=1)\n",
    "stack_prec = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=7, scoring=stack_prec_scorer)\n",
    "stack_rec_scorer = make_scorer(recall_score, average='macro', zero_division=1)\n",
    "stack_rec = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=7, scoring=stack_rec_scorer)\n",
    "stack_f1_scorer = make_scorer(f1_score, average='macro', zero_division=1)\n",
    "stack_f1 = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=7, scoring=stack_f1_scorer)\n",
    "stack_mae = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=7, scoring='neg_mean_absolute_error')\n",
    "stack_rmse = cross_val_score(estimator=stack_model, X=X_train, y=y_train, cv=7, scoring='neg_mean_squared_error')\n",
    "actual_mae = abs(stack_mae.mean())\n",
    "print(f\"stacked model Accuracy : {stack_acc.mean()}\")\n",
    "print(f\"stacked model Precision : {stack_prec.mean()}\")\n",
    "print(f\"stacked model Recall : {stack_rec.mean()}\")\n",
    "print(f\"stacked model F1-Score : {stack_f1.mean()}\")\n",
    "print(f\"stacked MAE : {actual_mae.mean()}\")\n",
    "print(f\"stacked RMSE : {sqrt(-stack_rmse.mean())}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLotting roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(classifiers, clf_names):\n\u001b[0;32m     16\u001b[0m     clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m---> 17\u001b[0m     y_prob \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(\u001b[43mX_test\u001b[49m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Combine the ROC curves for each class for this model\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     all_fpr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize the classifiers\n",
    "rf_clf = RandomForestClassifier()\n",
    "xgb_clf = XGBClassifier()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "lr_clf = LogisticRegression(max_iter=1000)\n",
    "svm_clf = SVC(probability=True)  # Set probability=True to enable probability estimates for SVC\n",
    "\n",
    "classifiers = [rf_clf, xgb_clf, dt_clf, lr_clf, svm_clf,stack_model]\n",
    "clf_names = ['Random Forest', 'XGBoost', 'Decision Tree', 'Logistic Regression', 'SVM','Stacked_model']\n",
    "\n",
    "# Dictionary to store the ROC curve data for each model\n",
    "roc_data = {name: {'fpr': [], 'tpr': [], 'roc_auc': 0.0} for name in clf_names}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for clf, name in zip(classifiers, clf_names):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "    # Combine the ROC curves for each class for this model\n",
    "    all_fpr = np.linspace(0, 1, 100)\n",
    "    mean_tpr = 0.0\n",
    "    for class_idx in range(len(np.unique(y_train))):\n",
    "        fpr, tpr, _ = roc_curve(y_test == class_idx, y_prob[:, class_idx])\n",
    "        mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
    "        roc_data[name]['fpr'].append(fpr)\n",
    "        roc_data[name]['tpr'].append(tpr)\n",
    "\n",
    "    mean_tpr /= len(np.unique(y_train))\n",
    "    roc_data[name]['tpr'] = mean_tpr\n",
    "    roc_data[name]['roc_auc'] = auc(all_fpr, mean_tpr)\n",
    "\n",
    "# ... (previous code up to initialization of classifiers)\n",
    "\n",
    "# ... (rest of the code for training and evaluating each model)\n",
    "\n",
    "plt.figure(figsize=(10, 6), facecolor='white')\n",
    "\n",
    "# Plot the combined ROC curve for each model\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "linestyles = ['-', '-', '-', '-', '-', '-']\n",
    "linewidths = [2, 2, 2, 2, 2, 2]\n",
    "for name, color, linestyle, linewidth in zip(clf_names, colors, linestyles, linewidths):\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    mean_tpr = roc_data[name]['tpr']\n",
    "    mean_auc = roc_data[name]['roc_auc']\n",
    "    plt.plot(mean_fpr, mean_tpr, color=color, linestyle=linestyle, linewidth=linewidth,\n",
    "             label=f'{name} ( AUC = {mean_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=2)  # Random guessing line with adjusted width\n",
    "plt.xlim([-0.02, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "#plt.title('Combined ROC Curves for Each Model')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Set the background to white\n",
    "fig = plt.gcf()\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.gca().set_facecolor('white')\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('roc_curves.png', dpi=1000, bbox_inches='tight', transparent=True, facecolor='white')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
